######################################################################
#
# Biocache configuration file
#
# This file has been generated via an ansible script.
#
######################################################################

# The email address of the technical contact
technical.contact=admin@mail.infrabas.se

# Whether to enable performance analysis using JMX
jmx.debug.enabled=true

# The base URL for biocache web services
webservices.root=http://biocacheservice:8080/biocache-service

# Cassandra Config
db=cassandra3
# cassandra hosts - this should be comma separated list in the case of a cluster
cassandra.hosts=cassandradb
cassandra.port=9042
local.node.ip=127.0.0.1
cassandra.pool=biocache-store-pool
cassandra.keyspace=occ
cassandra.max.connections=-1
cassandra.max.retries=6
thrift.operation.timeout=80000
cluster.size=1
node.number=0
cassandra.async.updates.enabled = false
cassandra.async.updates.threads = 16
cassandra.async.paging.enabled = false
cassandra.token.split = 10

# The number of concurrent Cassandra update threads
cassandra.update.threads= 16

# Record page size used by processing, indexing and other operations using a full table scan
cassandra.fetch.size= 500

# Database read timeout in milliseconds
cassandra.timeout= 120000

# Zookeeper address - used to retrieve SOLR configuration if not available locally
zookeeper.address =
zookeeper.updates.enabled = false

#######################################################
# File system usage
#######################################################

# Directory used to track the status of uploads
upload.status=/data/biocache-upload/status

# Directory used by sandbox functionality to write data files to be ingested
upload.temp=/data/biocache-upload/temp

# Base URL to media files
#media.url=http://localhost/biocache-media/

# Directory root for images
#media.dir=/data/biocache-media/

# Use local storage or an instance of image-service to store media
#media.store.local=false

# URL instance of image-service to store media
media.store.url=http://localhost/images

# Directory to log deleted row keys to
deleted.file.store=/data/biocache-delete/

# List tool endpoint
list.tool.url=http://localhost/specieslists

# Whether to enable list tool integration (this is used at indexing time only)
include.species.lists=true

# Whether or not to enable SDS checks
sds.enabled=false

# SDS data file
sds.url=

# The directory to write files to while ingesting data
load.dir=/data/biocache-load/

# Charts services
charts.facets.string.max= 1000
charts.facets.number.max= 1000

#######################################################
# External services
#######################################################

# The URL of SOLR services. In the case of embedded SOLR (no for production), this could be a directory path instead
# solrHome=http://localhost:8080/solr
# OR a list of Zookeeper nodes if the Solr configuration is stored in Zookeeper (advertised on port 2181)
# solrHome=zookeeper-1.example:2181,zookeeper-2.example:2181,zookeeper-3.ala:2181
solr.home=http://solr:8983/solr/biocache

# Solr HTTP Client Connection Pool configuration to avoid opening too many concurrent connections to the Solr server
solr.connection.pool.size= 50
solr.connection.pool.maxperroute= 50

# Solr HTTP Connection timeout defaults, in milliseconds (could have been overriden in internal Solr Client code in various places)
solr.connection.connecttimeout= 30000
solr.connection.requesttimeout= 30000
solr.connection.sockettimeout= 30000

# Solr HTTP Cache settings (requires you to setup HTTP Cache headers in your solr configuration before they will take effect)
solr.connection.cache.entries= 500
# Maximum object size to store in the cache, in bytes (Default 256 kilobytes: 1024 * 256 = 262144 bytes)
solr.connection.cache.object.size= 262144

# The number of concurrent Solr update threads
solr.update.threads=4

# The HTTP User Agent used for some queries from biocache-store
biocache.useragent=Biocache

# The SOLR collection to query
solr.collection=biocache

# Base URL for registry (collectory) web services
registry.url=http://collectory:8080/ws

# URL to use for retrieve a citations CSV for downloads
citations.url=http://collectory:8080/ws/citations

# API key to use to authenticate WS requests
registry.api.key=

# If enabled, processing & loading operations will cause an metadata update in the registry
allow.registry.updates=false

# Base URL for taxon services (BIE)
service.bie.ws.url=http://bieindex:8080/bie-index

# Base URL for taxon pages (BIE)
service.bie.ui.url=http://localhost/ala-bie

# Allow service to be disabled via config (enabled by default)
service.bie.enabled=false

# Base URL for Biocache UI
biocache.ui.url=http://localhost/ala-hub

# Base URL for DOI service DOI
doi.service.url=https://doi.ala.org.au/api/
doi.service.apiKey=
#Define API key in inventory file

#######################################################
# Miscellaneous configuration options
#######################################################

# The URL of layer services
layers.service.url=http://localhost/spatial-service

# To use layers service for sampling
layers.service.sampling=true
layers.service.download.sample=true

# Used by location processor for associating a country with an occurrence record where only stateProvince supplied and not coordinates are available
default.country=

# Specify fields to sample - set to 'none' for no sampling
sample.fields=

# The list of default fields to use if a list can not be obtained from the spatial layers.
default.sample.fields=

# geospatial layers used to add darwin core properties from lat/lng. uses IDs of layers from layer service
layer.state.province=
layer.terrestrial=
layer.marine=
layer.countries=
layer.localgov=

# Lucene indexes for name matching
name.index.dir=/data/lucene/namematching

# The languages to use for common names. Applicable for people using GBIF name matching indexes
commonname.lang= en

# Exclude sensitive values for the listed data resources (comma separated list)
exclude.sensitive.values=

# Additional fields to index (used by biocache-store only)
extra.misc.fields=
#extraMiscFields=OriginalSeedQuantity_i,AdjustedSeedQuantity_i,CurrentSeedQuantity_i,ViabilitySummary_d
additional.fields.to.index=dynamicProperties

# Max number of threads to use when processing a request
endemic.query.maxthreads= 30

# Max number of threads to use when processing a solr search query which is not an endemic query, or an online download or an offline download
solr.downloadquery.maxthreads= 30

# Max number of threads to use when processing an online download (occurrences/index/download)
online.downloadquery.maxthreads= 30

# Max number of threads to use when processing an offline download (occurrences/offline/download)
download.offline.parallelquery.maxthreads= 30

# An artificial throttle used between calls to Solr for paged results, including online and offline downloads
download.throttle.ms= 50

# The batch size for individual queries to Solr during downloads
download.batch.size= 500

# The size of an internal fixed length blocking queue used to parallelise reading from Solr before writing to this queue
download.internal.queue.size= 100

# Maximum total time for downloads to be execute. Defaults to 1 week (604,800,000ms)
download.max.execute.time= 604800000

# Maximum total time to wait for downloads to be written to disk after Solr queries are complete. Defaults to 5 minutes (300,000ms)
# Increase this if you are seeing messages about downloads being interrupted
# This needs to be fairly large as shapefiles are completely written to temp files after all of the Solr queries complete
download.max.completion.time= 300000

# Base directory for heatmap images
heatmap.output.dir=/data/output/heatmap

# AuthService properties to inject
auth.user.details.url=https://auth.bioatlas.se/userdetails/userDetails
auth.user.names.id.path=getUserList
auth.usernames.for.numeric.id.path=getUserListWithIds
auth.substitution.fields=assertion_user_id,user_id,alau_user_id

# Caches to enable/disable. Comment out the caches that you wish to enable
caches.auth.enabled=false
caches.log.enabled=false

# Note: The collections cache is problematic when collectory/biocache-service on deployed on the same tomcat
caches.collections.enabled=false
caches.layers.enabled=false

taxon.profile.cache.all=false

# Cache sizes can be lowered to reduce memory footprint at the possible cost of throughput
taxon.profile.cache.size=10000
classification.cache.size=10000
commonname.cache.size=10000
spatial.cache.size=10000
attribution.cache.size=10000
sensitivity.cache.size=10000
location.cache.size=10000

# Citations disable - for now we can disable them in the future will need a way to customise the source.
citations.enabled=true

# URL for retrieve list of contacts for collection
collection.contacts.url=http://collectory:8080/ws/collection

# URL for LoggerService
logger.service.url=http://localhost/logger-service/service/logger/

# Temporary working directory (used by processing)
tmp.work.dir=/data/tmp

# Restart Data Service
# Warning: Uses unsynchronised reflection to access private fields and save/restore them from disk
# Enable at your own risk
restart.data.enabled=false
restart.data.frequency=60000

#######################################################
# Volunteer portal (DigiVol) integration
#######################################################

# Hub ID for the DigiVol
volunteer.hub.uid=
# Data Provider ID for DigiVol
volunteer.dp.uid=

#######################################################
# SFTP integration
#######################################################

# User details to SFTP/SCP from upload
uploadUser=
uploadPassword=


#######################################################
# Facets
#######################################################

# Limit to &facets term count for all queries
facets.max=10

# Limit the default &facets term count. This limits the default facets assigned from facets.json
facets.defaultmax=0

# Default &facet value (true|false). Clients must always set &facet=true when facets are required and this default is false.
facet.default=true

# Autocomplete related caches, it will still work when these are disabled but the cached information will be unavailable.
autocomplete.species.images.enabled=true
autocomplete.species.counts.enabled=true
autocomplete.commonnames.extra.enabled=true

# Set SOLR batch size. Default=1000
solr.batch.size=500

# Set SOLR hard commit size. Default=10000 - used in indexing.
solr.hardcommit.size=5000

# Temporary directory where shapefiles are created for downloads
shapefile.tmp.dir=/data/biocache-download/tmp

# URL or path to subgroups JSON configuration
species.subgroups.url=/data/biocache/config/subgroups.json

#######################################################
# Downloads
#######################################################

# offline downloads email
download.email.subject:[hubName] Occurrence Download Complete - [filename]
download.auth.sensitive = false
download.solr.only=false

download.email.template=/data/biocache/config/download-email.html
download.readme.template=/data/biocache/config/download-readme.html

download.doi.licence.prefix=Datasets are covered by the following licence(s):
download.doi.title.prefix=Occurrence download
download.doi.landing.page.baseUrl=http://localhost/ala-hub/download/doi?doi=

download.doi.failure.message=A DOI was requested for this download however it could not be minted.

download.doi.email.template=/data/biocache/config/download-doi-email.html
download.doi.readme.template=/data/biocache/config/download-doi-readme.html

#DOI Default metadata
doi.author=Bioatlas Sweden
doi.description=BioAtlas occurrence record download
doi.resourceText=Species information

# Base URL for generated download files
download.url=http://biocacheservice:8080/biocache-service/biocache-download
download.dir=/data/biocache-download

# Download queue configuration
concurrent.downloads.json=[{\"label\": \"smallSolr\", \"threads\": 4, \"maxRecords\": 50000, \"type\": \"index\", \"pollDelay\": 10, \"executionDelay\": 10, \"threadPriority\": 5}, {\"label\": \"largeSolr\", \"threads\": 1, \"maxRecords\": 100000000, \"type\": \"index\", \"pollDelay\": 100, \"executionDelay\": 100, \"threadPriority\": 1}, {\"label\": \"smallCassandra\", \"threads\": 1, \"maxRecords\": 50000, \"type\": \"db\", \"pollDelay\": 10, \"executionDelay\": 10, \"threadPriority\": 5}, {\"label\": \"defaultUnrestricted\", \"threads\": 1, \"pollDelay\": 1000, \"executionDelay\": 100, \"threadPriority\": 1}]

# Maximum points in WKT string supported
qid.wkt.maxPoints=50000

# Maximum size of the WMS cache
wms.cache.size.max=1048576

# Default set of fields in the download - for the legacy format
default.download.fields=id,data_resource_uid,data_resource,institution_uid,institution_name,collection_uid,collection_name,license,catalogue_number,taxon_concept_lsid,raw_taxon_name,raw_common_name,taxon_name,rank,common_name,kingdom,phylum,class,order,family,genus,species,subspecies,institution_code,collection_code,locality,raw_latitude,raw_longitude,raw_datum,latitude,longitude,coordinate_precision,coordinate_uncertainty,country,state,cl959,min_elevation_d,max_elevation_d,min_depth_d,max_depth_d,individual_count,recorded_by,year,month,day,verbatim_event_date,basis_of_record,raw_basis_of_record,occurrence_status,sex,preparations,information_withheld,data_generalizations,outlier_layer,taxonomic_kosher,geospatial_kosher

# Maximum offline file size
download.offline.max.size=100000000

# Grid indexing enabled (used by biocache store only)
gridref.indexing.enabled=false

# National checklist GUID pattern
national.checklist.guid.pattern=.*

# Used in OGC XML services
organizationName=
orgCity=
orgStateProvince=
orgPostcode=
orgCountry=
orgPhone=
orgFax=
orgEmail=
